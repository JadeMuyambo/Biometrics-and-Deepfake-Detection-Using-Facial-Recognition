# -*- coding: utf-8 -*-
"""JadeMuyambo_CSC616_Biometrics_and_Deepfakes_FinalProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CuLhFYh6nJn1pcWv_8NuLtLfGoSJhy82
"""

# Jade Muyambo

#---------------------------------------------------------------------
# CSC616 Final Project
# Biometrics: The use of facial authentication for deepfake detection
#---------------------------------------------------------------------

# Importing Libraries
import time
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score

# Mounting Google Drive to acess dataset
from google.colab import drive
drive.mount('/content/drive')

# Defining the paths to the dataset
base_path = '/content/drive/My Drive/Colab Notebooks/Dataset_Kaggle/'
train_path = base_path + 'Train/'
test_path = base_path + 'Test/'
validation_path = base_path + 'Validation/'

# Data loading and augmentation
datagen = ImageDataGenerator(rescale=1./255)
train_generator = datagen.flow_from_directory(train_path, target_size=(256, 256), batch_size=32, class_mode='binary')
validation_generator = datagen.flow_from_directory(validation_path, target_size=(256, 256), batch_size=32, class_mode='binary')
test_generator = datagen.flow_from_directory(test_path, target_size=(256, 256), batch_size=32, class_mode='binary')

# Displaying dataset information in a table
data_info = pd.DataFrame({
    "Dataset": ["Train", "Validation", "Test"],
    "Number of Images": [train_generator.samples, validation_generator.samples, test_generator.samples],
    "Batch Size": [train_generator.batch_size, validation_generator.batch_size, test_generator.batch_size]
})
print(data_info)

# Model architecture
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),
    MaxPooling2D(2, 2),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# Training the CNN
history = model.fit(train_generator, epochs=3, validation_data=validation_generator)

# Modifying model for feature extraction
feature_extractor = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)

# Extracting and training SVM on features
train_features = feature_extractor.predict(train_generator)
train_labels = train_generator.labels
svm_classifier = SVC(kernel='linear')
svm_classifier.fit(train_features, train_labels)

# Evaluating the model using extracted features from the test set
test_features = feature_extractor.predict(test_generator)
test_labels = test_generator.labels
start_time = time.time()
predictions = svm_classifier.predict(test_features)
end_time = time.time()

# Calculating metrics
accuracy = accuracy_score(test_labels, predictions) * 100  # Convert to percentage
precision = precision_score(test_labels, predictions)
recall = recall_score(test_labels, predictions)
f1 = f1_score(test_labels, predictions)
cm = confusion_matrix(test_labels, predictions)
tn, fp, fn, tp = cm.ravel()
tnr = tn / (tn + fp)
fnr = fn / (fn + tp)
detection_speed = end_time - start_time

# Displaying the results
print(f"Accuracy: {accuracy:.2f}%")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")
print(f"True Negative Rate: {tnr:.2f}")
print(f"False Negative Rate: {fnr:.2f}")
print(f"Deepfake Detection Speed: {detection_speed:.2f} seconds")
print("Confusion Matrix:\n", cm)

# Displaying the confusion matrix as a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Visualizing results with adjusted font size for clarity
def plot_images(images, actual_labels, predicted_labels):
    plt.figure(figsize=(10, 10))
    for i in range(20):  # adjust number as needed (takes 20 images per run)
        plt.subplot(4, 5, i + 1)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)
        plt.imshow(images[i], cmap=plt.cm.binary)
        plt.xlabel(f"Actual: {actual_labels[i]}\nPredicted: {predicted_labels[i]}", fontsize=10)  # Adjust fontsize here
    plt.show()

# Load a batch of images from the test generator
test_images, test_labels = next(test_generator)

# Predicting using the model
predictions = svm_classifier.predict(feature_extractor.predict(test_images))

# Converting predictions from binary to labels
predicted_labels = ['Fake' if pred > 0.5 else 'Real' for pred in predictions]
actual_labels = ['Fake' if label > 0.5 else 'Real' for label in test_labels]

# Plotting the images with actual and predicted labels
plot_images(test_images, actual_labels, predicted_labels)